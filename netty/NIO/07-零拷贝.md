### 传统I/O的读写过程

![](img\2022-08-18 223739.png)

1. 应用发起读取请求，准备读取数据
2. 内核将数据从硬盘读到内核缓冲区中
3. 内核将数据拷贝到用户缓冲区中
4. 应用从用户缓冲区拿到数据 ，进行数据处理佳哥



### 详细的读写操作流程

![](img\2022-08-18 223740.png)



上图的几个概念



#### 应用程序

就是安装在操作系统上的各种应用。

#### 系统内核

系统内核是一些列计算机的核心资源的集合，不仅包括CPU、总线这些硬件设备，也包括进程管理、文件管理、内存管理、设备驱动、系统调用等一些列功能。

#### 外部存储

外部存储就是指硬盘、U盘等外部存储介质。

#### 内核态

- 内核态是操作系统内核运行的模式，当操作系统内核执行特权指令时，处于内核态。
- 在内核态下，操作系统内核拥有最高权限，可以访问计算机的所有硬件资源和敏感数据，执行特权指令，控制系统的整体运行。
- 内核态提供了操作系统管理和控制计算机硬件的能力，它负责处理系统调用、中断、硬件异常等核心任务。

#### 用户态

这里的用户可以理解为应用程序，这个用户是对于计算机的内核而言的，对于内核来说，系统上的各种应用程序会发出指令来调用内核的资源，这时候，应用程序就是内核的用户。

- 用户态是应用程序运行的模式，当应用程序执行普通的指令时，处于用户态。
- 在用户态下，应用程序只能访问自己的内存空间和受限的硬件资源，无法直接访问操作系统的敏感数据或控制计算机的硬件设备。
- 用户态提供了一种安全的运行环境，确保应用程序之间相互隔离，防止恶意程序对系统造成影响。

#### 模式切换

计算机为了安全性考虑，区分了内核态和用户态，应用程序不能直接调用内核资源，必须要切换到内核态之后，让内核来调用，内核调用完资源，再返回给应用程序，这个时候，系统在切换会用户态，应用程序在用户态下才能处理数据。

上述过程其实一次读和一次写都分别发生了两次模式切换。

![](img\2022-08-18 223741.png)



#### 内核缓冲区

内核缓冲区指内存中专门用来给内核直接使用的内存空间。可以把它理解为应用程序和外部存储进行数据交互的一个中间介质。

应用程序想要读外部数据，要从这里读。应用程序想要写入外部存储，要通过内核缓冲区。

#### 用户缓冲区

用户缓冲区可以理解为应用程序可以直接读写的内存空间。因为应用程序没法直接到内核读写数据， 所以应用程序想要处理数据，必须先通过用户缓冲区。

#### 磁盘缓冲区

磁盘缓冲区是计算机内存中用于暂存从磁盘读取的数据或将数据写入磁盘之前的临时存储区域。它是一种优化磁盘 I/O 操作的机制，通过利用内存的快速访问速度，减少对慢速磁盘的频繁访问，提高数据读取和写入的性能和效率。

#### PageCache

- PageCache 是 Linux 内核对文件系统进行缓存的一种机制。它使用空闲内存来缓存从文件系统读取的数据块，加速文件的读取和写入操作。
- 当应用程序或进程读取文件时，数据会首先从文件系统读取到 PageCache 中。如果之后再次读取相同的数据，就可以直接从 PageCache 中获取，避免了再次访问文件系统。
- 同样，当应用程序或进程将数据写入文件时，数据会先暂存到 PageCache 中，然后由 Linux 内核异步地将数据写入磁盘，从而提高写入操作的效率。



#### **page 与 Page Cache**

page 是内存管理分配的基本单位， Page Cache 由多个 page 构成

page 在操作系统中通常为 4KB 大小（32bits/64bits），而 Page Cache 的大小则为 4KB 的整数倍



#### Page Cache 与 buffer cache

**Linux会利用空闲的内存来做cached & buffers**，加速文件的读和写操作的

就是 Page Cache 就是加速用的，不是程序程序实实在在吃掉的内存，Buffers 是 Page Cache 的一部分

执行 free 命令

```
~ free -m
                          total       used       free     shared    buffers     cached
Mem:                      128956      96440      32515          0       5368      39900
-/+ buffers/cache:                    51172      77784
Swap:        			   16002          0      16001
```

-buffers/cache ：  used内存数--51172（指的第一部分Mem行中的used – buffers – cached）

​		**反映的是被程序实实在在吃掉的内存**

+buffers/cache  ： free内存数--77784（指的第一部分Mem行中的free + buffers + cached）

​		**反映的是可以挪用的内存总数**

cached ： 当前的页缓存（Page Cache）

buffers 列表示当前的块缓存（buffer cache）占用量

简单：**Page Cache 用于缓存文件的页数据，buffer cache 用于缓存块设备（如磁盘）的块数据**

注意：**Buffers 是 Page Cache 的一部分**



Page Cache 与 buffer cache 的共同目的都是加速数据 I/O：

​		写数据时首先写到缓存，将写入的页标记为 dirty，然后向外部存储 flush--（缓存写机制中的 write-back）

​		读数据时首先读取缓存，如果未命中，再去外部存储读取，并且将读取来的数据也加入缓存

操作系统总是积极地将所有空闲内存都用作 Page Cache 和 buffer cache，当内存不够用时也会用 LRU 等算法淘汰缓存页



#### **Page Cache 的优势**

**优势**：加快数据访问**，**减少 I/O 次数，提高系统磁盘 I/O 吞吐量

**劣势**：最直接的缺点是需要占用额外物理内存空间，物理内存在比较紧俏的时候可能会导致频繁的 swap 操作，最终导致系统的磁盘 I/O 负载的上升

​			另一个缺陷是对于应用层并没有提供很好的管理 API，应用层即使想优化 Page Cache 的使用策略也很难进行

​			因此一些应用选择在用户空间实现自己的 page 管理，例如 MySQL InnoDB 存储引擎以 16KB 的页进行管理



#### 脏页

Page Cache 内存中的内容与硬盘中的文件不一致就被标记为 dirty （脏页）



#### 当前 Linux 下以两种方式实现文件一致性：

1. **Write Through（写穿）**：向用户层提供特定接口，应用程序可主动调用接口来保证文件一致性；
2. **Write back（写回）**：系统中存在定期任务（表现形式为内核线程），周期性地同步文件系统中文件脏数据块，这是默认的 Linux 一致性方案；

上述两种方式最终都依赖于系统调用，主要分为如下三种系统调用：

![](img\2022-08-18 223742.png)



#### 再说数据读写操作流程

上面弄明白了这几个概念后，再回过头看一下那个流程图，是不是就清楚多了。

##### 读操作

1. 首先应用程序向内核发起读请求，这时候进行一次模式切换了，从用户态切换到内核态；
2. 内核向外部存储或网络套接字发起读操作；
3. 将数据写入磁盘缓冲区；
4. 系统内核将数据从磁盘缓冲区拷贝到内核缓冲区，顺便再将一份（或者一部分）拷贝到 PageCache；
5. 内核将数据拷贝到用户缓冲区，供应用程序处理。此时又进行一次模态切换，从内核态切换回用户态；

##### 写操作

1. 应用程序向内核发起写请求，这时候进行一次模式切换了，从用户态切换到内核态；
2. 内核将要写入的数据从用户缓冲区拷贝到 PageCache，同时将数据拷贝到内核缓冲区；
3. 然后内核将数据写入到磁盘缓冲区，从而写入磁盘，或者直接写入网络套接字。





### 瓶颈在哪里

**数据拷贝**

> 数据的传输通常涉及多次数据拷贝
>
> 读操作，内核将数据从磁盘缓冲区拷贝到内核缓冲区（顺便还拷贝到 PageCache），然后再再从内核缓冲区拷贝到用户缓冲区
>
> 写操作，数据需要从应用程序的用户缓冲区复制到内核缓冲区，然后再从内核缓冲区复制到设备或网络缓冲区

**用户态和内核态的切换**

> 由于数据要经过内核缓冲区，导致数据在用户态和内核态之间来回切换，切换过程中会有上下文的切换，如此一来，大大增加了处理数据的复杂性和时间开销。
>
> 每一次操作耗费的时间虽然很小，但是当并发量高了以后，积少成多，也是不小的开销。所以要提高性能、减少开销就要从以上两个问题下手了





### 零拷贝

这时候，零拷贝技术就出来解决问题了。



零拷贝的理想状态就是操作数据不用拷贝

但是正常情况下并不就是一次复制操作都没有，而是尽量减少拷贝操作的次数。

1. 减少数据在各个存储区域的复制操作，例如从磁盘缓冲区到内核缓冲区等；
2. 减少用户态和内核态的切换次数及上下文切换
3. 使用一些优化手段，例如对需要操作的数据先缓存起来，内核中的 PageCache 就是这个作用



### 实现零拷贝方案



#### 直接内存访问（DMA）

将与硬件打交道，拷贝速度最慢的地方交给DMA，释放CPU--（一种硬件设备，现在大部分设备都有这个功能）



DMA 是一种硬件特性，允许外设（如网络适配器、磁盘控制器等）直接访问系统内存，而无需通过 CPU 的介入。

**在进行 I/O 设备和内存的数据传输的时候，数据搬运的工作全部交给 DMA 控制器，**

**而 CPU 不再参与任何与数据搬运相关的事情，这样 CPU 就可以去处理别的事务**

![](img\2022-08-18 223743.png)



如上图所示，内核将数据读取的大部分数据读取操作都交个了 DMA 控制器，而空出来的资源就可以去处理其他的任务了。

CPU 在这个过程中还是必不可少的，因为传输什么数据，从哪里传输到哪里，还需要 CPU 来告诉 DMA 控制器。

整个过程如下：

![](img\2022-08-18 223744.png)







**Direct Memory Access** 技术虽然能释放了CPU

但是期间还是原来的情况：

**发生了 4 次用户态与内核态的上下文切换，4 次数据拷贝，其中两次是CPU 拷贝，另外最慢的两次则是通过 DMA 的拷贝的**

4 次上下文切换：

一次是 `read()` ，一次是 `write()`，每次系统调用都得先从用户态切换到内核态，等内核完成任务后，再从内核态切换回用户态。

4 次数据拷贝

- *第一次拷贝*，把磁盘上的数据拷贝到操作系统内核的缓冲区里，这个拷贝的过程是通过 DMA 搬运的。
- *第二次拷贝*，把内核缓冲区的数据拷贝到用户的缓冲区里，于是我们应用程序就可以使用这部分数据了，这个拷贝到过程是由 CPU 完成的。
- *第三次拷贝*，把刚才拷贝到用户的缓冲区里的数据，再拷贝到内核的 socket 的缓冲区里，这个过程依然还是由 CPU 搬运的。
- *第四次拷贝*，把内核的 socket 缓冲区里的数据，拷贝到网卡的缓冲区里，这个过程又是由 DMA 搬运的。



#### 如何实现零拷贝

零拷贝技术实现的方式通常

- sendfile
- 共享内存
- mmap + write



#### **sendfile**

在 Linux 内核版本 2.1 中，提供了一个专门发送文件的系统调用函数 `sendfile()`

又在 Linux 内核 2.4 版本开始起，支持网卡 SG-DMA 技术的情况下

实现文件复制（网络数据转发）过程中的零拷贝：

**没有在内存层面去拷贝数据，全程没有通过 CPU 来搬运数据，所有的数据都是通过 DMA 来进行传输的**

**sendfile ：**

可以直接将文件数据从文件系统传输到网络套接字或者目标文件，而无需经过用户缓冲区和内核缓冲区。

![](img\2022-08-18 223745.png)

sendfile 函数实现的文件传输方式相比传统文件传输的方式，减少了 2 次上下文切换和数据拷贝次数

**只需要 2 次上下文切换和数据拷贝次数，就可以完成文件的传输，**

**而且 2 次的数据拷贝过程，都不需要通过 CPU，2 次都是由 DMA 来搬运。**

在文件的复制和网络的转发中，**可以把文件传输的性能提高至少一倍以上**

**使用零拷贝技术的项目**

Kafka 这个开源项

Nginx 也支持零拷贝技术，一般默认是开启零拷贝技术，这样有利于提高文件传输的效率，是否开启零拷贝技术的配置如下：
`text http { ... sendfile on ... }`



如果不用sendfile，如果将A文件复制到B文件。一次read() ，一次write()

1. 需要先将A文件的数据拷贝到内核缓冲区，再从内核缓冲区拷贝到用户缓冲区；
2. 然后内核再将用户缓冲区的数据拷贝到内核缓冲区，之后才能写入到B文件；



注意：要使用 sendfile，Linux 内核版本必须要 2.1 以上的版本





#### 共享内存

使用共享内存技术，应用程序和内核可以共享同一块内存区域，避免在用户态和内核态之间进行数据拷贝。

应用程序可以直接将数据写入共享内存，然后内核可以直接从共享内存中读取数据进行传输，或者反之。

![](img\2022-08-18 223746.png)



#### 内存映射文件（Memory-mapped Files）

内存映射文件直接将磁盘文件映射到应用程序的地址空间，使得应用程序可以直接在内存中读取和写入文件数据，这样一来，对映射内容的修改就是直接的反应到实际的文件中。

当文件数据需要传输时，内核可以直接从内存映射区域读取数据进行传输，避免了数据在用户态和内核态之间的额外拷贝。

虽然看上去感觉和共享内存没什么差别，但是两者的实现方式完全不同，一个是共享地址，一个是映射文件内容。







### Java 实现零拷贝的方式

java 标准的 IO 库是没有零拷贝方式的实现的，标准IO就相当于上面所说的传统模式。

只是在 Java 推出的 NIO 中，才包含了一套新的 I/O 类，如 `ByteBuffer` 和 `Channel`，它们可以在一定程度上实现零拷贝



`ByteBuffer`：可以直接操作字节数据，避免了数据在用户态和内核态之间的复制。

`Channel`：支持直接将数据从文件通道或网络通道传输到另一个通道，实现文件和网络的零拷贝传输。





#### 传统 IO 写法

```java
public static void ioCopy() {
  try {
    File sourceFile = new File(SOURCE_FILE_PATH);
    File targetFile = new File(TARGET_FILE_PATH);
    try (FileInputStream fis = new FileInputStream(sourceFile);
         FileOutputStream fos = new FileOutputStream(targetFile)) {
      byte[] buffer = new byte[1024];
      int bytesRead;
      while ((bytesRead = fis.read(buffer)) != -1) {
        fos.write(buffer, 0, bytesRead);
      }
    }
    System.out.println("传输 " + formatFileSize(sourceFile.length()) + " 字节到目标文件");
  } catch (IOException e) {
    e.printStackTrace();
  }
}
```





#### FileChannel.transferTo() 和 transferFrom()

两个方法首选用 sendfile 方式，只要当前操作系统支持，就用 sendfile，例如Linux或MacOS。

如果系统不支持，例如windows，则采用内存映射文件的方式实现。



#### transferTo()

```java
public static void nioTransferTo() {
  try {
    File sourceFile = new File(SOURCE_FILE_PATH);
    File targetFile = new File(TARGET_FILE_PATH);
    try (FileChannel sourceChannel = new RandomAccessFile(sourceFile, "r").getChannel();
         FileChannel targetChannel = new RandomAccessFile(targetFile, "rw").getChannel()) {
      long transferredBytes = sourceChannel.transferTo(0, sourceChannel.size(), targetChannel);

      System.out.println("传输 " + formatFileSize(transferredBytes) + " 字节到目标文件");
    }
  } catch (IOException e) {
    e.printStackTrace();
  }
}
```

#### transferFrom()

```java
public static void nioTransferFrom() {
  try {
    File sourceFile = new File(SOURCE_FILE_PATH);
    File targetFile = new File(TARGET_FILE_PATH);

    try (FileChannel sourceChannel = new RandomAccessFile(sourceFile, "r").getChannel();
         FileChannel targetChannel = new RandomAccessFile(targetFile, "rw").getChannel()) {
      long transferredBytes = targetChannel.transferFrom(sourceChannel, 0, sourceChannel.size());
      System.out.println("传输 " + formatFileSize(transferredBytes) + " 字节到目标文件");
    }
  } catch (IOException e) {
    e.printStackTrace();
  }
}
```



transferTo()` 和 `transferFrom() 都是文件数据从一个通道传输到另一个通道

只是参数位置的不同，底层实现首选用 sendfile 方式，不支持则用内存映射文件的方式实现



#### **Memory-Mapped Files**

Java 的 NIO 也支持内存映射文件（Memory-mapped Files），通过 `FileChannel.map()` 实现。

```java
  public static void nioMap(){
        try {
            File sourceFile = new File(SOURCE_FILE_PATH);
            File targetFile = new File(TARGET_FILE_PATH);

            try (FileChannel sourceChannel = new RandomAccessFile(sourceFile, "r").getChannel();
                 FileChannel targetChannel = new RandomAccessFile(targetFile, "rw").getChannel()) {
                long fileSize = sourceChannel.size();
                MappedByteBuffer buffer = sourceChannel.map(FileChannel.MapMode.READ_ONLY, 0, fileSize);
                targetChannel.write(buffer);
                System.out.println("传输 " + formatFileSize(fileSize) + " 字节到目标文件");
            }
        } catch (IOException e) {
            e.printStackTrace();
        }
    }

```





